{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Stable Diffusion\n",
    "\n",
    "This project brings stable diffusion models to web browsers. **Everything runs inside the browser with no server support.** To our knowledge, this is the world’s first stable diffusion completely running on the browser. This notebook is a walk-through on the entire pipeline, including how to import the stable diffusion models from Hugging Face and PyTorch, how to optimize and build the model locally, and how to deploy it to native GPU runtime and WebGPU runtime. Now let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](site/img/fig/workflow.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "To import and build the model, we first need to install the on-going development of TVM Unity and other dependencies with the following pip command.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install --pre torch --upgrade --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "!python3 -m pip install -r requirements.txt\n",
    "\n",
    "has_gpu = !nvidia-smi -L\n",
    "cudav = \"-cu116\" if has_gpu else \"\" # check https://mlc.ai/wheels if you have a different CUDA version\n",
    "\n",
    "!python3 -m pip install mlc-ai-nightly{cudav} -f https://mlc.ai/wheels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import necessary packages and set up the artifact directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import system\n",
    "\n",
    "import tvm\n",
    "from tvm import relax\n",
    "from tvm.relax.frontend.torch import dynamo_capture_subgraphs\n",
    "from tvm.relax.frontend.torch import from_fx\n",
    "from tvm.script import relax as R\n",
    "\n",
    "import torch\n",
    "from torch import fx\n",
    "\n",
    "from web_stable_diffusion import trace\n",
    "from web_stable_diffusion import utils\n",
    "\n",
    "torch_dev_key = \"cpu\"\n",
    "\n",
    "if system() == \"Darwin\":\n",
    "    target = tvm.target.Target(\"apple/m1-gpu\")\n",
    "    device = tvm.metal()\n",
    "else:\n",
    "    target = tvm.target.Target(\"cuda\" if has_gpu else \"llvm\")\n",
    "    device = tvm.cuda() if has_gpu else tvm.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import stable diffusion models\n",
    "\n",
    "With necessary packages imported, the first step is to import the stable diffusion PyTorch models into TVM. A standard text-to-image stable diffusion pipeline consists of four stages:\n",
    "* A text tokenizer which converts the input text prompts to tokens.\n",
    "* A text encoder (CLIP) which encodes the tokenized text prompts to text embeddings.\n",
    "* A denoising model (UNet) which denoises a random initial latents for a certain number of steps, guided by the encoded text.\n",
    "* An image decoder (VAE) which decodes the final latents to an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](site/img/fig/pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall import these models from PyTorch to TVM. As in Web Stable Diffusion we leverage the [wasm port](https://blog.mithrilsecurity.io/porting-tokenizers-to-wasm/) of the Hugging Face [tokenizers library](https://github.com/huggingface/tokenizers), we only need to import the rest three models: CLIP, UNet and VAE.\n",
    "\n",
    "PyTorch provides [TorchDynamo](https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html) that can capture the computational graph of a torch model and represent the graph in [Torch FX](https://pytorch.org/docs/stable/fx.html), torch’s graph-level intermediate representation (IR) of a model. TorchDynamo and FX are the tools we leverage. More specifically, we first use TorchDynamo to capture the model’s execution into an FX GraphModule, and then translate the FX GraphModule to a Relax function -- the graph-level IR of TVM Unity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared with building and deploying the stable diffusion model to, for eaxmple, local CUDA backend, building and deploying the model to web with WebGPU runtime is special, mostly because of the difference of runtime environment:\n",
    "\n",
    "**Web runtime has no Python (let alone PyTorch, NumPy). We can only leverage JavaScript for model deployment on web.**\n",
    "\n",
    "Considering this major difference, we will need to _simplify the runtime stable diffusion pipeline (written in JavaScript) as much as possible_, and maximize the number of tasks in the build stage, ahead of the web model deployment. Our workflow demonstrates this principal in the two following aspects:\n",
    "1. capture more computation to TVM’s IRModule,\n",
    "2. separate models’ weights from the IRModule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Capture more computation to TVM’s IRModule\n",
    "\n",
    "Because we do not have PyTorch in web runtime, it is necessary to cover those additional operations in our IRModule as well. There are two different approaches to the same goal. Both are very simple:\n",
    "1. implement the operations manually with existing Relax infrastructure, or\n",
    "2. write a wrapper `torch.nn.Module` which both contains the ML model and the appending/prepending operations.\n",
    "\n",
    "In our practice, we adopt both approaches. For some operations we use wrapper `nn.Module`, while for others we write the operation manually. Let’s walk through each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ①. The CLIP text encoder\n",
    "\n",
    "In the entire pipeline, the text encoder is used twice: one for the prompt and the other one for the negative prompt. Since it is next to the tokenization (which we do not import) and is followed by the concatenation of both prompts’ embeddings, the encoder is a standalone phase. Therefore, we use an `nn.Module` to wrap the single CLIP forward.\n",
    "\n",
    "Relax has provided a helper function `dynamo_capture_subgraphs`, which uses TorchDynamo to capture the computational graph, and translate the captured FX GraphModule to a Relax function. In stable diffusion, the maximum length of text prompts is 77. So the input to the CLIP model has length 77. We create a random input as the simulation of tokenized text prompt, and pass it to the wrapper for TorchDynamo capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_to_text_embeddings(pipe) -> tvm.IRModule:\n",
    "    # Define the wrapper torch.nn.Module for CLIP.\n",
    "    class CLIPModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, clip):\n",
    "            super().__init__()\n",
    "            self.clip = clip\n",
    "\n",
    "        def forward(self, text_input_ids):\n",
    "            text_embeddings = self.clip(text_input_ids)[0]\n",
    "            return text_embeddings\n",
    "\n",
    "    clip = pipe.text_encoder\n",
    "    clip_to_text_embeddings = CLIPModelWrapper(clip)\n",
    "\n",
    "    # Create random input (77 is the maximum length).\n",
    "    text_input_ids = torch.rand((1, 77)).to(torch.int32)\n",
    "    # Capture CLIP's computational graph.\n",
    "    mod = dynamo_capture_subgraphs(\n",
    "        clip_to_text_embeddings.forward,\n",
    "        text_input_ids,\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    assert len(mod.functions) == 1\n",
    "\n",
    "    return tvm.IRModule({\"clip\": mod[\"subgraph_0\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ②. The embedding concatenation\n",
    "\n",
    "This stage concatenates the embeddings of both prompts, and is followed by the huge UNet iteration. It is also standalone, and here we choose to implement the concatenation by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_embeddings() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "    cond_embeddings = relax.Var(\"cond_embeddings\", R.Tensor([1, 77, 768], \"float32\"))\n",
    "    uncond_embeddings = relax.Var(\n",
    "        \"uncond_embeddings\", R.Tensor([1, 77, 768], \"float32\")\n",
    "    )\n",
    "    with bb.function(\"concat_embeddings\", [cond_embeddings, uncond_embeddings]):\n",
    "        res = bb.emit(\n",
    "            relax.op.concat([cond_embeddings, uncond_embeddings], axis=0)\n",
    "        )\n",
    "        bb.emit_func_output(res)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③. Latent concat + UNet + classifier-free guidance\n",
    "\n",
    "The third stage is the first part of the UNet loop body. It is mostly the UNet forward, while before the UNet forward there is a step of latent concatenation, and after UNet forward, one step of [classifier-free guidance](https://github.com/huggingface/diffusers/blob/79eb3d07d07a2dada172c5958d6fca478c860f16/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L674-L677) will be performed to force the generation to better match the prompt potentially. Since the latent concatenation and the guidance are immediately before/after the UNet forward which we always have to import whatever, we use a wrapper `nn.Module` to import all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_latents_to_noise_pred(pipe, device_str: str) -> tvm.IRModule:\n",
    "    class UNetModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, unet):\n",
    "            super().__init__()\n",
    "            self.unet = unet\n",
    "            # Default guidance scale factor in stable diffusion.\n",
    "            self.guidance_scale = 7.5\n",
    "\n",
    "        def forward(self, latents, timestep_tensor, text_embeddings):\n",
    "            # Latent concatenation.\n",
    "            latent_model_input = torch.cat([latents] * 2, dim=0)\n",
    "            # UNet forward.\n",
    "            noise_pred = self.unet(latent_model_input, timestep_tensor, text_embeddings)\n",
    "            # Classifier-free guidance.\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + self.guidance_scale * (\n",
    "                noise_pred_text - noise_pred_uncond\n",
    "            )\n",
    "            return noise_pred\n",
    "\n",
    "    unet = utils.get_unet(pipe, device_str)\n",
    "    unet_to_noise_pred = UNetModelWrapper(unet)\n",
    "    graph = fx.symbolic_trace(unet_to_noise_pred)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 64, 64), \"float32\"), ((), \"int32\"), ((2, 77, 768), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"unet\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ④. Scheduler step\n",
    "\n",
    "The scheduler step stage is the other part of the UNet iteration, and is very important for updating the latents towards the denoising direction. There are many kinds of different schedulers, with each having (possibly) largely different implementation. Here we use the multi-step DPM solver scheduler.\n",
    "\n",
    "One feature of schedulers is that schedulers usually maintain a list of history UNet output, and the scheduler step operation takes the maintained history as input internally. Since the step operation is history dependent, we are not able to combine the scheduler step together with the previous UNet part, and have to implement it separately. Because the scheduler step is also standalone mostly, we implement it by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dpm_solver_multistep_scheduler_steps() -> tvm.IRModule:\n",
    "    bb = relax.BlockBuilder()\n",
    "\n",
    "    # convert_model_output, the first function in multi-step DPM solver.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    alpha = relax.Var(f\"alpha\", R.Tensor((), \"float32\"))\n",
    "    sigma = relax.Var(f\"sigma\", R.Tensor((), \"float32\"))\n",
    "    with bb.function(\n",
    "        \"dpm_solver_multistep_scheduler_convert_model_output\",\n",
    "        [sample, model_output, alpha, sigma],\n",
    "    ):\n",
    "        converted_model_output = bb.emit(\n",
    "            (sample - sigma * model_output) / alpha, \"converted_model_output\"\n",
    "        )\n",
    "        bb.emit_func_output(converted_model_output)\n",
    "\n",
    "    # step, the second function.\n",
    "    sample = relax.Var(\"sample\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    model_output = relax.Var(\"model_output\", R.Tensor((1, 4, 64, 64), \"float32\"))\n",
    "    last_model_output = relax.Var(\n",
    "        \"last_model_output\", R.Tensor((1, 4, 64, 64), \"float32\")\n",
    "    )\n",
    "    consts = [relax.Var(f\"c{i}\", R.Tensor((), \"float32\")) for i in range(3)]\n",
    "\n",
    "    with bb.function(\n",
    "        \"dpm_solver_multistep_scheduler_step\",\n",
    "        [sample, model_output, last_model_output, *consts],\n",
    "    ):\n",
    "        prev_sample = bb.emit(\n",
    "            consts[0] * sample\n",
    "            - consts[1] * model_output\n",
    "            - consts[2] * (model_output - last_model_output),\n",
    "            \"prev_sample\",\n",
    "        )\n",
    "        bb.emit_func_output(prev_sample)\n",
    "\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑤. VAE + image normalization\n",
    "\n",
    "The last but one stage is the VAE step followed by an image normalization, which normalizes the value range from `[-1, 1]` to integers in `[0, 255]`. For the same reason as ③, we use a wrapper `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_to_image(pipe) -> tvm.IRModule:\n",
    "    class VAEModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            # Scale the latents so that it can be decoded by VAE.\n",
    "            latents = 1 / 0.13025 * latents\n",
    "            # VAE decode\n",
    "            # z = self.vae.post_quant_conv(latents)\n",
    "            image = self.vae.decode(latents, return_dict=False)[0]\n",
    "            # Image normalization\n",
    "            image = (image / 2 + 0.5).clamp(min=0, max=1)\n",
    "            image = (image.permute(0, 2, 3, 1) * 255).round()\n",
    "            return image\n",
    "\n",
    "    vae = utils.get_vae(pipe, \"1.5\")\n",
    "    vae_to_image = VAEModelWrapper(vae)\n",
    "\n",
    "    graph = fx.symbolic_trace(vae_to_image)\n",
    "    mod = from_fx(\n",
    "        graph,\n",
    "        [((1, 4, 64, 64), \"float32\")],\n",
    "        keep_params_as_input=True,\n",
    "    )\n",
    "    return tvm.IRModule({\"vae\": mod[\"main\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⑥. Image conversion to RGBA\n",
    "\n",
    "To display the image, we need to convert the image to RGBA mode that can be directly rendered by the web runtime. This conversion requires dtype `uint32`, which PyTorch doesn’t support. Therefore, we are unable to combine this stage with the previous one, and need to implement it by hand with Relax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_rgba() -> tvm.IRModule:\n",
    "    from tvm import te\n",
    "\n",
    "    def f_image_to_rgba(A):\n",
    "        def fcompute(y, x):\n",
    "            return (\n",
    "                A[0, y, x, 0].astype(\"uint32\")\n",
    "                | (A[0, y, x, 1].astype(\"uint32\") << 8)\n",
    "                | (A[0, y, x, 2].astype(\"uint32\") << 16)\n",
    "                | tvm.tir.const(255 << 24, \"uint32\")\n",
    "            )\n",
    "\n",
    "        return te.compute((512, 512), fcompute, name=\"image_to_rgba\")\n",
    "\n",
    "    bb = relax.BlockBuilder()\n",
    "    x = relax.Var(\"x\", R.Tensor([1, 512, 512, 3], \"float32\"))\n",
    "    with bb.function(\"image_to_rgba\", [x]):\n",
    "        image = bb.emit(\n",
    "            bb.call_te(f_image_to_rgba, x, primfunc_name_hint=\"tir_image_to_rgba\")\n",
    "        )\n",
    "        bb.emit_func_output(image)\n",
    "    return bb.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine every piece together\n",
    "\n",
    "We have described how we import every part of the stable diffusion pipeline into Relax. Now we can combine every of them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"SG161222/Realistic_Vision_V5.1_noVAE\")\n",
    "clip = clip_to_text_embeddings(pipe)\n",
    "unet = unet_latents_to_noise_pred(pipe, torch_dev_key)\n",
    "vae = vae_to_image(pipe)\n",
    "concat_embeddings = concat_embeddings()\n",
    "image_to_rgba = image_to_rgba()\n",
    "schedulers = [\n",
    "    dpm_solver_multistep_scheduler_steps(),\n",
    "    trace.PNDMScheduler.scheduler_steps()\n",
    "]\n",
    "\n",
    "mod: tvm.IRModule = utils.merge_irmodules(\n",
    "    clip,\n",
    "    unet,\n",
    "    vae,\n",
    "    concat_embeddings,\n",
    "    image_to_rgba,\n",
    "    *schedulers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Separate models’ weights from the IRModule\n",
    "\n",
    "After the steps above, we get an IRModule which contains the computational graph of the stable diffusion model, as well as the weights of the model. To reduce the size of the built artifact so that it can be universally deployed everywhere (including the web), we separate models’ weights from the IRModule we get. For a weight tensor, we use a placeholder to represent it in the IRModule, instead of letting it reside in the IRModule as a constant tensor. We will save the separated weights to the disk later. At the beginning of the deployment, we will load these weights from disk to memory.\n",
    "\n",
    "The separation is implemented as function `relax.frontend.detach_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod, params = relax.frontend.detach_params(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to print out the entire IRModule via \n",
    "```python\n",
    "mod.show()\n",
    "```\n",
    "to see the models and other functions we have imported in the IRModule. The output will be thousands of lines long, so we do not run it live here. If you try it out, the printed output should look in the following way:\n",
    "```python\n",
    "# from tvm.script import ir as I\n",
    "# from tvm.script import relax as R\n",
    "\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def clip(\n",
    "        inp_0: R.Tensor((1, 77), dtype=\"int32\"),\n",
    "        self_clip_text_model_embeddings_position_embedding_weight: R.Tensor((77, 768), dtype=\"float32\"),\n",
    "        self_clip_text_model_embeddings_token_embedding_weight: R.Tensor((49408, 768), dtype=\"float32\"),\n",
    "        ...\n",
    "    ) -> R.Tensor((1, 77, 768), dtype=\"float32\"):\n",
    "        R.func_attr({\"num_input\": 1})\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((1, 77), dtype=\"int32\") = R.reshape(inp_0, R.shape([1, 77]))\n",
    "            lv1: R.Tensor((1, 77), dtype=\"int32\") = R.astype(lv, dtype=\"int32\")\n",
    "            lv2: R.Tensor((77,), dtype=\"int32\") = R.reshape(lv1, R.shape([77]))\n",
    "            lv3: R.Tensor((77, 768), dtype=\"float32\") = R.take(self_clip_text_model_embeddings_token_embedding_weight, lv2, axis=0)\n",
    "            lv4: R.Tensor((1, 77, 768), dtype=\"float32\") = R.reshape(lv3, R.shape([1, 77, 768]))\n",
    "            lv5: R.Tensor((1, 77), dtype=\"int32\") = R.astype(metadata[\"relax.expr.Constant\"][0], dtype=\"int32\")\n",
    "            lv6: R.Tensor((77,), dtype=\"int32\") = R.reshape(lv5, R.shape([77]))\n",
    "            lv7: R.Tensor((77, 768), dtype=\"float32\") = R.take(self_clip_text_model_embeddings_position_embedding_weight, lv6, axis=0)\n",
    "            lv8: R.Tensor((1, 77, 768), dtype=\"float32\") = R.reshape(lv7, R.shape([1, 77, 768]))\n",
    "            lv9: R.Tensor((1, 77, 768), dtype=\"float32\") = R.add(lv4, lv8)\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of printing out the whole IRModule, what we can do is to print out the names of the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_relax_funcnames(mod: tvm.IRModule):\n",
    "    for global_var, func in mod.functions.items():\n",
    "        if isinstance(func, relax.Function):\n",
    "            print(global_var.name_hint)\n",
    "    print()\n",
    "\n",
    "\n",
    "print_relax_funcnames(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out one of the weight tensors to see what we have captured for model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first weight parameter of the CLIP model.\n",
    "params[\"clip\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, we have went through all steps of importing the stable diffusion model to Relax. The above logic of import is implemented in [`web_stable_diffusion/trace/model_trace.py`](https://github.com/mlc-ai/web-stable-diffusion/blob/main/web_stable_diffusion/trace/model_trace.py) and [`web_stable_diffusion/trace/scheduler_trace.py`](https://github.com/mlc-ai/web-stable-diffusion/blob/main/web_stable_diffusion/trace/scheduler_trace.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model\n",
    "\n",
    "So far, we have imported all the desired models and functions in the stable diffusion into an IRModule of TVM. This section is about how we transform and optimize the IRModule we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default optimization pipeline in Relax\n",
    "\n",
    "Relax provides a default optimization pipeline for a given IRModule. This pipeline mainly includes the constant folding optimization and the kernel fusion optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the default optimization pipeline.\n",
    "mod = relax.pipeline.get_pipeline()(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the kernel fusion optimization in the default optimization pipeline, there will be some unused elements in the IRModule. Here we do a clean up on the IRModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"clip\", \"unet\", \"vae\"]\n",
    "scheduler_func_names = [\n",
    "    name\n",
    "    for name in trace.DPMSolverMultistepScheduler.scheduler_steps_func_names()\n",
    "]\n",
    "entry_funcs = (\n",
    "    model_names + scheduler_func_names + [\"image_to_rgba\", \"concat_embeddings\"]\n",
    ")\n",
    "\n",
    "# Clean up unused parts of the IRModule.\n",
    "mod = relax.transform.DeadCodeElimination(entry_funcs)(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift computation on weight parameters\n",
    "\n",
    "Since the stable diffusion models’ weight parameter are always constants at the time of deployment, we can finish the computation around the weight parameters in ahead of deployment. To this end, we separate such computation from the original functions to standalone functions in the IRModule, so that such computation can be done in the build stage. The separation is implemented as the `LiftTransformParams` pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = relax.transform.LiftTransformParams()(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that after applying this pass, three new functions are inside the IRModule. Their names are ended with `\"_transform_params\"`, which means the functions are in the purpose of computing models’ weight parameters and will be built and executed in the build stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for global_var, function in mod.functions.items():\n",
    "    if isinstance(function, relax.Function):\n",
    "        if global_var.name_hint.endswith(\"_transform_params\"):\n",
    "            print(\n",
    "                global_var.name_hint,\n",
    "                f' # <=== This is the weight parameter computation function for \"{global_var.name_hint[:-17]}\"',\n",
    "            )\n",
    "        else:\n",
    "            print(global_var.name_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the IRModule at build stage and deployment stage\n",
    "\n",
    "After last subsection, the IRModule now contains functions both for the build stage and deployment stage. Therefore, we need to split the IRModule into two parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_transform, mod_deploy = utils.split_transform_deploy_mod(\n",
    "    mod, model_names, entry_funcs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s print the Relax function names in the IRModules after split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"In IRModule for build stage:\")\n",
    "print_relax_funcnames(mod_transform)\n",
    "\n",
    "print(\"In IRModule for deployment stage:\")\n",
    "print_relax_funcnames(mod_deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for build\n",
    "\n",
    "We have finished transforming the IRModule. One last step before building the IRModule is to compute and save the all the constants at deployment to our artifact directory. There are two kinds of constants that we need to compute and save:\n",
    "* the constants of the models’ weight parameters and related computation lifted by pass `LiftTransformParams`, and\n",
    "* the coefficients used by the stable diffusion schedulers at each UNet iteration.\n",
    "\n",
    "Compared with computing them in the deployed model, doing the computation in ahead has several benefits:\n",
    "* it minimizes the workload of the deployed model,\n",
    "* we can universally and easily deploy our model on platforms without, e.g., Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save the scheduler constants.\n",
    "trace.compute_save_scheduler_consts(artifact_path=\"dist\")\n",
    "# Compute and save the models's weight parameters.\n",
    "new_params = utils.transform_params(mod_transform, params)\n",
    "utils.save_params(new_params, artifact_path=\"dist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All transformation and the final build preparation in this section are contained in the `legalize_and_lift_params` function of `build.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "After all preparation for the build stage, in this section we will build the model. The contents of this section is implemented as the `build` function of `build.py`. To build the model outside this notebook, the command is\n",
    "```shell\n",
    "python3 build.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply provided kernel optimization database\n",
    "\n",
    "The IRModule before the build stage contains three kinds of Relax functions:\n",
    "* Relax functions of ML models (CLIP, UNet and VAE),\n",
    "* Relax functions of schedulers (scheduler step function), and\n",
    "* some utility Relax functions (e.g., text embedding concatenation, VAE-output-to-image conversion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the Relax function calls into many low-level primitive tensor functions. For example, the first scheduler step Relax function of PNDM scheduler is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_deploy[\"dpm_solver_multistep_scheduler_step\"].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, this scheduler step Relax function calls into two multiplication functions, one division function and one subtraction function. We can print out the multiplication function to see how the primitive tensor functions look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "called_gv = mod_deploy[\"dpm_solver_multistep_scheduler_step\"].body.blocks[0].bindings[0].value.args[0]\n",
    "\n",
    "mod_show = tvm.IRModule({called_gv: mod_deploy[called_gv]})\n",
    "mod_show.show(black_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To have decently high performance for all such primitive tensor functions, we have provided a pre-tuned database, which contains the optimization for every primitive tensor functions in the IRModule to build. What we need to do here is simply to apply the database to the IRModule, with the help of function `MetaScheduleApplyDatabase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm import meta_schedule as ms\n",
    "\n",
    "db = ms.database.create(work_dir=\"log_db\")\n",
    "with target, db, tvm.transform.PassContext(opt_level=3):\n",
    "    mod_deploy = relax.transform.MetaScheduleApplyDatabase()(mod_deploy)\n",
    "    mod_deploy = tvm.tir.transform.DefaultGPUSchedule()(mod_deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the database, the multiplication primitive tensor function shown above now becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_show = tvm.IRModule({called_gv: mod_deploy[called_gv]})\n",
    "mod_show.show(black_format=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "\n",
    "With the database applied in the last subsection, every part of the IRModule is ready to be built. We now use `relax.build` to build the IRModule to the desired backend target, e.g., Metal for Mac M1. `relax.build` returns an executable object, which will be taken and executed by the VirtualMachine of Relax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = relax.build(mod=mod_deploy, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the executable to disk as a shared library. The exported shared library will be loaded back in the deployment stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.export_library(\"dist/stable_diffusion.so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build stage right ends here. Let’s recall a bit: after build, the objects we saved to the disk are:\n",
    "* the shared library, which is the build artifact of the IRModule to deploy,\n",
    "* the weight parameter constants,\n",
    "* the scheduler constants.\n",
    "\n",
    "With merely these parts, we are able to bring and deploy our stable diffusion model to anywhere that supports minimum TVM runtime. This well demonstrates our concept of universal deployment.\n",
    "\n",
    "Of course, we still need a stable diffusion pipeline script that connects all these parts together and describes the procedure of stable diffusion. We will introduce this in the next section -- deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model locally\n",
    "\n",
    "This section will demo how we deploy the built model locally with native GPU runtime. The contents of this section is implemented in `deploy.py`. To deploy the model outside this notebook, the command is\n",
    "```shell\n",
    "python3 deploy.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "The first job of deployment is to load back the model shared library and the constants we saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weight parameters back.\n",
    "const_params_dict = utils.load_params(artifact_path=\"dist\", device=device)\n",
    "# Load the model executable back from the shared library.\n",
    "ex = tvm.runtime.load_module(\"dist/stable_diffusion.so\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `relax.VirtualMachine` with the executable object loaded back and the targeted device (Metal/CUDA/CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = relax.VirtualMachine(rt_mod=ex, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The virtual machine provides interfaces to call into the Relax functions in `mod_deploy` above, which we will use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the scheduler class\n",
    "\n",
    "Similar to scheduler classes in Hugging Face diffusers, we also define the scheduler class for stable diffusion runtime. On construction, the scheduler class will load the scheduler constants from the JSON file we saved to the disk at the build stage. The main interface of the scheduler class is the `step` function, which is invoked at the end of each UNet iteration.\n",
    "\n",
    "We implement the PNDM scheduler, which is the default scheduler of [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from web_stable_diffusion import runtime\n",
    "\n",
    "\n",
    "class DPMSolverMultistepScheduler(runtime.Scheduler):\n",
    "    scheduler_name = \"multistep-dpm-solver\"\n",
    "\n",
    "    def __init__(self, artifact_path: str, device) -> None:\n",
    "        # Load the scheduler constants.\n",
    "        with open(\n",
    "            f\"{artifact_path}/scheduler_dpm_solver_multistep_consts.json\", \"r\"\n",
    "        ) as file:\n",
    "            jsoncontent = file.read()\n",
    "        scheduler_consts = json.loads(jsoncontent)\n",
    "\n",
    "        def f_convert(data, dtype):\n",
    "            return [tvm.nd.array(np.array(t, dtype=dtype), device) for t in data]\n",
    "\n",
    "        self.timesteps = f_convert(scheduler_consts[\"timesteps\"], \"int32\")\n",
    "        self.alpha = f_convert(scheduler_consts[\"alpha\"], \"float32\")\n",
    "        self.sigma = f_convert(scheduler_consts[\"sigma\"], \"float32\")\n",
    "        self.c0 = f_convert(scheduler_consts[\"c0\"], \"float32\")\n",
    "        self.c1 = f_convert(scheduler_consts[\"c1\"], \"float32\")\n",
    "        self.c2 = f_convert(scheduler_consts[\"c2\"], \"float32\")\n",
    "\n",
    "        # Initialize the model_output history.\n",
    "        self.last_model_output: tvm.nd.NDArray = tvm.nd.empty(\n",
    "            (1, 4, 64, 64), \"float32\", device\n",
    "        )\n",
    "\n",
    "    def step(\n",
    "        self,\n",
    "        vm: relax.VirtualMachine,\n",
    "        model_output: tvm.nd.NDArray,\n",
    "        sample: tvm.nd.NDArray,\n",
    "        counter: int,\n",
    "    ) -> tvm.nd.NDArray:\n",
    "        # Invoke the functions through VM.\n",
    "        model_output = vm[\"dpm_solver_multistep_scheduler_convert_model_output\"](\n",
    "            sample, model_output, self.alpha[counter], self.sigma[counter]\n",
    "        )\n",
    "        prev_latents = vm[\"dpm_solver_multistep_scheduler_step\"](\n",
    "            sample,\n",
    "            model_output,\n",
    "            self.last_model_output,\n",
    "            self.c0[counter],\n",
    "            self.c1[counter],\n",
    "            self.c2[counter],\n",
    "        )\n",
    "        self.last_model_output = model_output\n",
    "        return prev_latents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the stable diffusion pipeline\n",
    "\n",
    "We now define the stable diffusion pipeline that connects everything together. The pipeline takes the virtual machine we build, the Hugging Face tokenizer, a scheduler and the model weight parameter constants for construction. The main interface of the pipeline takes a prompt and an optional negative prompt as input, and returns the generated image as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPTokenizer\n",
    "\n",
    "\n",
    "class TVMSDPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vm: relax.VirtualMachine,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        scheduler: runtime.Scheduler,\n",
    "        tvm_device,\n",
    "        param_dict,\n",
    "    ):\n",
    "        def wrapper(f, params):\n",
    "            def wrapped_f(*args):\n",
    "                return f(*args, *params)\n",
    "\n",
    "            return wrapped_f\n",
    "\n",
    "        self.vm = vm\n",
    "        self.clip_to_text_embeddings = wrapper(vm[\"clip\"], param_dict[\"clip\"])\n",
    "        self.unet_latents_to_noise_pred = wrapper(vm[\"unet\"], param_dict[\"unet\"])\n",
    "        self.vae_to_image = wrapper(vm[\"vae\"], param_dict[\"vae\"])\n",
    "        self.concat_embeddings = vm[\"concat_embeddings\"]\n",
    "        self.image_to_rgba = vm[\"image_to_rgba\"]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.scheduler = scheduler\n",
    "        self.tvm_device = tvm_device\n",
    "        self.param_dict = param_dict\n",
    "\n",
    "    def __call__(self, prompt: str, negative_prompt: str = \"\"):\n",
    "        # The height and width are fixed to 512.\n",
    "\n",
    "        # Compute the embeddings for the prompt and negative prompt.\n",
    "        list_text_embeddings = []\n",
    "        for text in [negative_prompt, prompt]:\n",
    "            text = [text]\n",
    "            # Tokenize the text.\n",
    "            text_inputs = self.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,  # 77\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            text_input_ids = text_inputs.input_ids.to(torch.int32)\n",
    "            # Clip the text if the length exceeds the maximum allowed length.\n",
    "            if text_input_ids.shape[-1] > self.tokenizer.model_max_length:\n",
    "                text_input_ids = text_input_ids[:, : self.tokenizer.model_max_length]\n",
    "\n",
    "            # Compute text embeddings.\n",
    "            text_input_ids = tvm.nd.array(text_input_ids.cpu().numpy(), self.tvm_device)\n",
    "            text_embeddings = self.clip_to_text_embeddings(text_input_ids)\n",
    "            list_text_embeddings.append(text_embeddings)\n",
    "        \n",
    "        # Concatenate the text embeddings.\n",
    "        text_embeddings = self.concat_embeddings(*list_text_embeddings)\n",
    "\n",
    "        # Randomly initialize the latents.\n",
    "        latents = torch.randn(\n",
    "            (1, 4, 64, 64),\n",
    "            device=\"cpu\",\n",
    "            dtype=torch.float32,\n",
    "        )\n",
    "        latents = tvm.nd.array(latents.numpy(), self.tvm_device)\n",
    "\n",
    "        # UNet iteration.\n",
    "        for i in tqdm(range(len(self.scheduler.timesteps))):\n",
    "            t = self.scheduler.timesteps[i]\n",
    "            noise_pred = self.unet_latents_to_noise_pred(latents, t, text_embeddings)\n",
    "            latents = self.scheduler.step(self.vm, noise_pred, latents, i)\n",
    "\n",
    "        # VAE decode.\n",
    "        image = self.vae_to_image(latents)\n",
    "\n",
    "        # Transform generated image to RGBA mode.\n",
    "        image = self.image_to_rgba(image)\n",
    "        return Image.fromarray(image.numpy().view(\"uint8\").reshape(512, 512, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "We are ready to go! Let’s instantiate the pipeline and pass in a real prompt to generate the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TVMSDPipeline(\n",
    "    vm=vm,\n",
    "    tokenizer=CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\"),\n",
    "    scheduler=runtime.DPMSolverMultistepScheduler(artifact_path=\"dist\", device=device),\n",
    "    tvm_device=device,\n",
    "    param_dict=const_params_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"high quality, face portrait photo of 30 y.o asian woman, wearing black shirt, serious face, detailed face, skin pores, cinematic shot, dramatic lighting\"\n",
    "neg_prompt = \"(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck\"\n",
    "\n",
    "start = time.time()\n",
    "image = pipe(prompt, negative_prompt=neg_prompt)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Time elapsed: {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `display` to showcase the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy on web\n",
    "\n",
    "We have tried to deploy the stable diffusion model with native GPU runtime. Now let’s try to deploy the stable diffusion to web end with WebGPU runtime.\n",
    "\n",
    "To deploy to web, everything is following the same procedure. Some minor differences are:\n",
    "* we need a build the model to WebGPU backend, instead of Metal backend,\n",
    "* we need to implement the stable diffusion pipeline and the scheduler runtime in JavaScript.\n",
    "\n",
    "Nevertheless, they both have the same spirit as before -- just in another form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install prerequisites\n",
    "\n",
    "We first install some prerequisite packages.\n",
    "\n",
    "1. [emscripten](https://emscripten.org). It is an LLVM-based compiler which compiles C/C++ source code to WebAssembly.\n",
    "    - Follow the [installation instruction](https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended) to install the latest emsdk.\n",
    "    - Source `emsdk_env.sh` by `source path/to/emsdk_env.sh`, so that `emcc` is reachable from PATH and the command `emcc` works.\n",
    "2. [Rust](https://www.rust-lang.org/tools/install).\n",
    "3. [`wasm-pack`](https://rustwasm.github.io/wasm-pack/installer/). It helps build Rust-generated WebAssembly, which used for tokenizer in our case here.\n",
    "4. Install jekyll by following the [official guides](https://jekyllrb.com/docs/installation/). It is the package we use for website.\n",
    "5. Install jekyll-remote-theme by command\n",
    "    ```shell\n",
    "    gem install jekyll-remote-theme\n",
    "    ```\n",
    "6. Install [Chrome Canary](https://www.google.com/chrome/canary/). It is a developer version of Chrome that enables the use of WebGPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the following commands to verify that we have installed them properly and can found these programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!emcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jekyll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wasm-pack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After verifying the functionality of these commands, we run `scripts/prep_deps.sh` to prepare all the necessary dependencies for the web build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TVM_HOME=3rdparty/tvm\n",
    "./scripts/prep_deps.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The execution `scripts/prep_deps.sh` is expected to end with\n",
    "```\n",
    "Copy /path/to/tvm/web/dist/tvmjs.bundle.js to dist\n",
    "Copy /path/to/tvm/web/dist/wasm/tvmjs_runtime.wasi.js to dist\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model to WebGPU backend\n",
    "\n",
    "We now build the model to WebGPU backend and export the executable to disk in the WebAssembly file format.\n",
    "\n",
    "In the previous section on building the model to native GPU backend, the command was\n",
    "```shell\n",
    "python3 build.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build to WebGPU backend, we still use `build.py`, just with an argument specifying the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export TVM_HOME=3rdparty/tvm\n",
    "python3 build.py --target webgpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the website\n",
    "\n",
    "The last thing to do is setting up the site by running the following command in a terminal session:\n",
    "```shell\n",
    "./scripts/local_deploy_site.sh\n",
    "```\n",
    "\n",
    "Once the website is set up, open `localhost:8888/` in Chrome Canary to try out the demo on your local machine!\n",
    "\n",
    "---\n",
    "\n",
    "_Remark: don’t forget to use_\n",
    "```shell\n",
    "/Applications/Google\\ Chrome\\ Canary.app/Contents/MacOS/Google\\ Chrome\\ Canary --enable-dawn-features=disable_robustness\n",
    "```\n",
    "_to launch Chrome Canary to turn off the robustness check from Chrome._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
